{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "year = 2023\n",
    "# Paths\n",
    "input_dir = \"Data/sp500_10k_links\"\n",
    "output_dir = \"Data/sp500_10k_items\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Safa Berber berbersafa123@gmail.com\"}\n",
    "\n",
    "# Sections to extract\n",
    "item_sections = [\n",
    "    (\"ITEM 1. BUSINESS\", \"ITEM 1A. RISK FACTORS\", \"Item_1\"),\n",
    "    (\"ITEM 1A. RISK FACTORS\", \"ITEM 1B. UNRESOLVED STAFF COMMENTS\", \"Item_1A\"),\n",
    "    (\"ITEM 7. MANAGEMENT‚ÄôS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\",\n",
    "     \"ITEM 7A. QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\", \"Item_7\"),\n",
    "    (\"ITEM 8. FINANCIAL STATEMENTS AND SUPPLEMENTARY DATA\",\n",
    "     \"ITEM 9. CHANGES IN AND DISAGREEMENTS WITH ACCOUNTANTS ON ACCOUNTING AND FINANCIAL DISCLOSURE\", \"Item_8\"),\n",
    "]\n",
    "\n",
    "def normalize_title(title):\n",
    "    # Normalize Unicode, replace special characters, and collapse whitespace\n",
    "    title = unicodedata.normalize(\"NFKD\", title)\n",
    "    title = title.replace(\"\\xa0\", \" \").replace(\"‚Äô\", \"'\")\n",
    "    title = re.sub(r'\\s+', ' ', title).strip().upper()\n",
    "    title = re.sub(r\"[‚Äê‚Äë‚Äí‚Äì‚Äî‚àí]\", \".\", title) \n",
    "    \n",
    "    # Remove unwanted punctuation \n",
    "    title = re.sub(r\"[^\\w\\s']\", '', title)\n",
    "    # Remove trailing extraneous words like \"NONE\", \"NOT APPLICABLE\", or \"TABLE OF CONTENTS\"\n",
    "    title = re.sub(r'\\s+NONE$', '', title)\n",
    "    title = re.sub(r'\\s+NOT APPLICABLE$', '', title)\n",
    "    title = re.sub(r'\\s+TABLE OF CONTENTS$', '', title)\n",
    "    # Remove any parenthetical text \n",
    "    title = re.sub(r'\\(.*?\\)', '', title).strip()\n",
    "    title = title.rstrip(\".:,;\")\n",
    "    \n",
    "    # Replace plurals with singular forms \n",
    "    replacements = {\n",
    "        \"DISCLOSURES\": \"DISCLOSURE\", \"STATEMENTS\": \"STATEMENT\", \"MATTERS\": \"MATTER\",\n",
    "        \"OPERATIONS\": \"OPERATION\", \"ESTIMATES\": \"ESTIMATE\", \"RESULTS\": \"RESULT\",\n",
    "        \"PROCEEDINGS\": \"PROCEEDING\", \"CONDITIONS\": \"CONDITION\", \"RISKS\": \"RISK\",\n",
    "        \"FACTORS\": \"FACTOR\", \"CONTROLS\": \"CONTROL\", \"PROPERTIES\": \"PROPERTY\",\n",
    "        \"RELATIONSHIPS\": \"RELATIONSHIP\", \"TRANSACTIONS\": \"TRANSACTION\",\n",
    "        \"SERVICES\": \"SERVICE\", \"DISCUSSIONS\": \"DISCUSSION\", \"EXHIBITS\": \"EXHIBIT\",\n",
    "        \"SCHEDULES\": \"SCHEDULE\", \"COMMENTS\": \"COMMENT\", \"ITEMS\": \"ITEM\",\n",
    "    }\n",
    "    for plural, singular in replacements.items():\n",
    "        title = re.sub(rf'\\b{plural}\\b', singular, title)\n",
    "        \n",
    "    # --- Item 7 Change Only ---\n",
    "    if title.startswith(\"ITEM 7\") and \"DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION\" in title:\n",
    "        title = \"ITEM 7 MANAGEMENTS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS\"\n",
    "    \n",
    "    # For headers \"ITEM 1 AND 2. BUSINESS AND PROPERTIES\"\n",
    "    if title.startswith(\"ITEM 1 AND 2\") and \"BUSINESS\" in title:\n",
    "        title = \"ITEM 1 BUSINESS\"\n",
    "        \n",
    "    # For headers \"ITEM 1. BUSINESS GENERAL\" or \"ITEM 1. BUSINESS OVERVIEW\"\n",
    "    if title.startswith(\"ITEM 1 BUSINESS\"):\n",
    "        title = \"ITEM 1 BUSINESS\"\n",
    "        \n",
    "    return title\n",
    "\n",
    "def extract_sections(html_text, item_pairs):\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    for tag in soup.find_all(['u', 'b', 'strong', 'span', 'i', 'em']):\n",
    "        if tag.string and tag.parent:\n",
    "            tag.unwrap()\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.replace(\"‚Äô\", \"'\").replace(\"\\xa0\", \" \")\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Split text into lines and filter out lines that are solely page numbers or exactly \"TABLE OF CONTENTS\"\n",
    "    lines = text.splitlines()\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if re.fullmatch(r'\\d+', stripped):\n",
    "            continue\n",
    "        if stripped.upper() == \"TABLE OF CONTENTS\":\n",
    "            continue\n",
    "        filtered_lines.append(line)\n",
    "        \n",
    "    cutoff = int(len(filtered_lines) * 0.04)\n",
    "    main_text = \"\\n\".join(filtered_lines[cutoff:])\n",
    "\n",
    "    # Fix headers broken across lines (e.g., \"ITEM 1.\\nBusiness\")\n",
    "    main_text = re.sub(\n",
    "        r'(ITEMS?\\s+\\d{1,2}[A-Z]?[.\\:\\-]?)\\s*\\n\\s*([^\\n]{2,200})',\n",
    "        r'\\1 \\2',\n",
    "        main_text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Modified regex to capture headers flexibly including optional \"and 2\"\n",
    "    section_pattern = re.compile(\n",
    "        r'^\\s*(items?\\s*\\d{1,2}(?:\\s*and\\s*2)?[a-z]?\\s*[.\\:\\-‚Äî]?\\s*[^\\n]{2,150})\\s*$',\n",
    "        re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    section_map = {}\n",
    "    for match in section_pattern.finditer(main_text):\n",
    "        raw_header = match.group(1).strip()\n",
    "        cleaned = normalize_title(raw_header)\n",
    "\n",
    "        # Check next ~200 characters after header to detect empty sections\n",
    "        section_start = match.start()\n",
    "        peek_ahead = main_text[section_start:section_start + 200].strip()\n",
    "        peek_lines = peek_ahead.splitlines()\n",
    "\n",
    "        # Just normalize \"None\" or \"Not Applicable\" sections to valid headers (not skip them)\n",
    "        if len(peek_lines) >= 2:\n",
    "            second_line = peek_lines[1].strip().upper()\n",
    "            second_line_cleaned = re.sub(r'[.]', '', second_line)\n",
    "            second_line_cleaned = second_line_cleaned.strip()\n",
    "            if second_line in {\"NONE\", \"NOT APPLICABLE\"}:\n",
    "                section_map[cleaned] = match.start()\n",
    "\n",
    "        if \"TABLE OF CONTENTS\" in cleaned:\n",
    "            continue\n",
    "        if 10 < len(cleaned) < 200:\n",
    "            section_map[cleaned] = section_start\n",
    "\n",
    "    sorted_sections = sorted(section_map.items(), key=lambda x: x[1])\n",
    "\n",
    "    extracted = {}\n",
    "    for start_title, end_title, col_name in item_pairs:\n",
    "        start_key = normalize_title(start_title)\n",
    "        end_key = normalize_title(end_title)\n",
    "\n",
    "        start_idx = section_map.get(start_key)\n",
    "        # For Item_1A: if the expected header is not found, try a fallback based on \"ITEM 1A RISK FACTOR\"\n",
    "        if col_name == \"Item_1A\" and start_key not in section_map:\n",
    "            for key in section_map:\n",
    "                if key.startswith(\"ITEM 1A RISK FACTOR\"):\n",
    "                    start_key = key\n",
    "                    break\n",
    "\n",
    "        start_idx = section_map.get(start_key)\n",
    "        end_idx = section_map.get(end_key)\n",
    "\n",
    "        # Fallback for the end header: if an exact match for end_key isn't found, use the next header starting with end_key after start_idx\n",
    "        if end_idx is None and start_idx is not None:\n",
    "            for key, pos in section_map.items():\n",
    "                if key.startswith(end_key) and pos > start_idx:\n",
    "                    end_idx = pos\n",
    "                    break\n",
    "\n",
    "        # If no valid end index is found or it comes before start, use the next header position\n",
    "        if start_idx is not None:\n",
    "            if end_idx is None or end_idx <= start_idx:\n",
    "                following_starts = [pos for k, pos in sorted_sections if pos > start_idx]\n",
    "                end_idx = following_starts[0] if following_starts else len(main_text)\n",
    "\n",
    "            if end_idx is not None and end_idx > start_idx:\n",
    "                content = main_text[start_idx:end_idx].strip()\n",
    "            else:\n",
    "                content = main_text[start_idx:start_idx + 1000].strip()\n",
    "                next_match = re.search(r'\\n\\s*ITEM\\s+\\d', content, re.IGNORECASE)\n",
    "                if next_match:\n",
    "                    content = content[:next_match.start()]\n",
    "            extracted[col_name] = content\n",
    "        else:\n",
    "            extracted[col_name] = f\"[NOT FOUND] {start_title} ‚Üí {end_title}\"\n",
    "    return extracted\n",
    "\n",
    "for year in range(year, year+1):\n",
    "    input_csv = os.path.join(input_dir, f\"filtered_10K_filings_{year}.csv\")\n",
    "    output_csv = os.path.join(output_dir, f\"items_filtered_10K_filings_{year}.csv\")\n",
    "\n",
    "    if not os.path.exists(input_csv):\n",
    "        print(f\"‚ö†Ô∏è Skipping {year}: File not found.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüìÇ Processing year: {year}\")\n",
    "    df = pd.read_csv(input_csv)\n",
    "    extracted_items = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        url = row['document_url']\n",
    "        company = row.get('company', f\"[Row {idx}]\")\n",
    "        print(f\"[{idx+1}/{len(df)}] Fetching: {url}\")\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            if resp.status_code == 200:\n",
    "                result = extract_sections(resp.text, item_sections)\n",
    "            else:\n",
    "                result = {sec[2]: f\"HTTP ERROR {resp.status_code}\" for sec in item_sections}\n",
    "        except Exception as e:\n",
    "            result = {sec[2]: f\"ERROR: {e}\" for sec in item_sections}\n",
    "\n",
    "        for col, val in result.items():\n",
    "            if isinstance(val, str) and val.startswith(\"[NOT FOUND]\"):\n",
    "                print(f\"  ‚ùå {company} - {col} missing: {val}\")\n",
    "\n",
    "        extracted_items.append(result)\n",
    "\n",
    "    items_df = pd.concat([df.reset_index(drop=True), pd.DataFrame(extracted_items)], axis=1)\n",
    "    items_df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Done! Saved: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
